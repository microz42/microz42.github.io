---
layout  : wiki
title   : ch10 - cnn, 합성곱 신경망
summary : 
date    : 2025-06-20 15:38:50 +0900
updated : 2025-06-30 15:14:12 +0900
tag     : cnn 합성곱 convolution
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
resource: CD919C05-D96F-3D51-612F-454D9D5A6198
---
* TOC
{:toc}

이미지 데이터에 특화된 CNN

## 왜 이미지는 FC(일반 신경망)으로 처리하면 안 될까?
기존에 배운 완전 연결(Fully connected) 신경망에 이미지를 그냥 때려 넣으면 몇 가지 심각한 문제가 터진다.
1. 파라미터 폭발
	- 224x224짜리 컬러 이미지는 입력값이 15만개가 넘는다. 여기에 FC 레이어 하나만 붙여도 파라미터가 수십억개가 되버린다. 이 많은 걸 언제 다 학습시키고, 어느 세월에 메모리에 올릴까?
2. 공간적 정보 무시
	- FC 네트워크는 입력 데이터를 그냥 길다란 1차원 벡터로 취급한다. 이미지 왼쪽 위 픽셀이랑 오른쪽 아래 픽셀이 옆 동네인지 태평양 건너인지 전혀 구분하지 못한다. 픽셀 순서를 다 섞어서 넣어도 똑같이 학습할 정도니 말 다 했다.
3. 위치에 따른 비효율성
	- 고양이 사진을 왼쪽으로 살짝 옮기면, FC 네트워크 입장에서는 완전히 새로운 데이터다. 왼쪽 고양이 따로, 오른쪽 고양이 따로, 가운데 고양이 따로... 전부 처음부터 다시 배워야 한다. 매우매우 비효율적.

CNN은 위 문제들을 해결하기 위해 태어난 이미지 처리 특화 아키텍처다.


## CNN 핵심 아이디어: 슬라이딩 필터
CNN의 모든 마법은 convolution 연산에서 시작된다.
- kernel 또는 filter: 3x3이나 5x5 같은 작은 행렬이다. **"특징 탐지기"**라고 생각하면 이해하기 쉽다. 어떤 커널은 수직선을 어떤 커널은 녹색 점박이를, 또 다른 커널은 둥근 모서리를 찾아내는 식으로.
- 파라미터 공유(parameter sharing): CNN의 가장 중요한 특징. 이 kernel/filter/특징 탐지기 하나를 가지고 이미지 전체를 쭉 훑고 지나간다. 이미지의 왼쪽 위에서 수직선을 찾던 커널이, 오른쪽 아래에서도 똑같은 방식으로 수직선을 찾는다. 덕분에 파라미터 수가 미친 듯이 줄어들고, 객체가 어데이 있든 같은 방식으로 인식할 수 있게 된다. 이게 **Translation Invariance**의 시작.
- Local connectivity(지역적 연결): 각 뉴런이 이미지의 전체 픽셀이 아닌, 커널이 훑고 지나가는 작은 지역(local region)의 픽셀에만 연결된다. 이미지의 "이웃" 관계를 자연스럽게 반영한다.

## CNN 아키텍처
보통 CNN은 Conv > ReLU > Pooling 블록을 레고처럼 여러 층 쌓아서 만든다.

### Pooling = 핵심만 보고!
convolution 연산을 한 뒤에 풀링이라는 과정으로 feature map의 사이즈를 줄여준다.
- 왜? 계산량을 줄이고, 모델이 위치 변화에 좀 더 둔감해지도록(불변성을 갖도록) 만들기 위해서
- Max Pooling: 2x2 같은 작은 영역에서 가장 큰 값(가장 활성화된 특징) 하나만 남기고 나머지는 버린다. "이 구역에 내가 찾던 특징이 조금이라도 있는지?"라고 묻는 것과 같다. 덕분에 객체가 1,2 픽셀 움직여도 결과가 크게 바뀌지 않게 된다.

### Receptive Field(깊어질수록 똑똑해지는 이유)
CNN 레이어를 계속 쌓으면 어떤 일이 일어날까?
- 1층: 작은 3x3 커널이 이미지의 아주 작은 부분(엣지, 코너 등)만 본다
- 2층: 1층의 특징 맵을 다시 3x3 커널로 훑고, 2층의 뉴런 하나는 1층의 3x3 영역을 보고, 그 1층 뉴런들은 원본 이미지의 3x3 영역을 봤으니, 결과적으로 2층 뉴런 하나는 원본 이미지의 더 넓은 5x5 영역을 보게 되는 셈이다.
- 결론: 레이어가 깊어질수록 뉴런 하나가 볼 수 있는 원본 이미지의 영역, 즉 Receptive field가 점점 넓어진다. 이를 통해 네트워크는 엣지와 색 같은 단순한 특징을 조합해 눈, 코를 만들고, 이걸 또 조합해 얼굴이라는 복잡한 특징을 학습하게 된다.

## CNN으로 할 수 있는 거
위의 구조를 바탕으로 컴퓨터 비전의 문제들을 해결한다
- Image classification: 이미지에 '고양이', '개'처럼 라벨 붙이는 작업
	- AlexNet: 2012년 등장. 딥러닝 시대를 연 모델. Conv-Pool 구조를 깊게 쌓고 마지막에 FC 레이어를 붙여 클래스를 예측.
	- VGG: "더 깊게!"를 외치며 3x3의 작은 커널을 엄청나게 쌓아 성능을 끌어올린 모델.
- Object detection: 이미지 안에서 여러 객체의 위치를 bounding box로 찾아내는 작업
	- YOLO(You Only Look Once): 이미지를 한번만 보고 객체의 종류와 위치를 동시에 예측하는 빠른 속도로 유명
- Semenatic segmentation: 이미지의 모든 픽셀을 "이 픽셀은 하늘", "저 픽셀은 사람"과 같이 분류하는, 가장 정교한 작업. 보통 이미지를 압축(encode)했다가 다시 원래 크기로 복원(decode)하는 "모래시계" 모양의 구조를 많이 쓴다.

## 용어
- Stride: 커널이 한 번에 몇 칸씩 건너뛰여 움직일지 정하는 값. stride=2로 주면 feature map의 크기가 절반으로 줄어든다(다운샘플링)
- padding: 커널이 이미지 가장자리를 처리할 때 크기가 줄어드는 걸 막기 위해, 이미지 테두리에 0 같은 값을 둘러준다.
- channels: "특징 탐지기"는 보통 하나만 쓰지 않는다. 수직선 탐지기, 둥근 모서리 탐지기, 빨간색 탐지기 등 수십, 수백 개의 커널을 동시에 돌린다. 이때 각 커널이 만들어내는 결과물 하나하나를 채널 또는 feature map이라고 부른다. 컬러 이미지가 R,G,B 3개의 채널로 시작하는 것과 같다.

## 정리
- CNN은 local conectivity와 parameter sharing 덕분에 이미지 처리에 가성비 갑인 모델이다
- conv-pool 블록을 깊게 쌓아 계층적 특징(hierarchical feature)을 학습한다. 얕은 층은 엣지, 깊은 층은 얼굴 같은 복잡한 걸 본다
- AlexNet이 불을 지폈고, YGG, YOLO 등이 그 뒤를 이어 분류, 탐지 등 컴퓨터 비전의 역사를 새로 썼다.
