---
layout  : wiki
title   : ch11 - ResNet
summary : 
date    : 2025-06-30 15:22:19 +0900
updated : 2025-07-01 11:24:09 +0900
tag     : resnet
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
resource: 2C899F17-A8C2-A474-828F-0BC3B947298B
---
* TOC
{:toc}

10장의 VGG보다 깊게 쌓았더니 오히려 성능이 떨어지는 미스터리.

## 더 깊게 쌓았는데 왜 더 멍쳥해지는지? 
VGG가 19층까지 쌓아서 성능을 올리는 거 보고, 50층, 100층, 1000층 쌓으면 더 똑똑해지겠네?  

결과는 참담.

단순히 레이어를 20층에서 56층으로 늘렸더니, 테스트 성능은 물론이고 training 성능까지 떨어지는 현상이 발생했다. 이건 오버피팅 문제도 아니였고 그냥 모델 자체가 학습을 제대로 못 한다는거다.(pg.188)

왜?

주범으로 지목되는 현상이 바로 shattered gradients다. 네트워크가 너무 깊어지면, 초기 레이어의 파라미터를 살짝만 바꿔도 loss 값이 완전히 예측 불가능하게 널뛰기를 한다. 손실 함수 표면이 완만한 언덕이 아니라, 아주 작은 자갈밭처럼 변해버려서 옵티마이저가 길을 잃고 헤매는 것.

## Residual Connection
이때 등장한 아이디어가 바로

**Residual connection(잔차 연결) 또는 Skip connection(스킵 연결)**이다.

아이디어는 매우 간단한데,
- 기존 방식: 출력 = F(입력) (입력을 받아 완전히 새롭게 변환)
- ResNet 방식: 출력 = 입력 + F(입력)

기존 네트워크가 입력을 받아 완전히 새로운 출력을 '창조'했다면, ResNet의 레이어(F)는 입력을 어떻게 '수정'할지만 배우면 된다. `입력`에 더해지는 변화량 또는 잔차(residual)만 학습하는 것.

### 이게 왜 그렇게 강력할까?
- 그래디언트 고속도로 뚫기: `출력 = 입력 + F(입력)` 구조 덕분에, 역전파 시 그래디언트가 `+` 브랜치를 타고 아무런 방해 없이 초기 레이어까지 쭉 흘러갈 수 있는 "고속도로"가 생긴다. 덕분에 깊은 곳까지 그래디언트가 깨지지 않고 전달되고, 손실 함수 표면도 훨씬 부드러워져 학습이 안정적으로 이뤄진다.
- 얕은 모델 앙상블 효과: 네트워크를 풀어보면, 사실상 수많은 얕은 네트워크들의 앙상블처럼 동작하는 걸로 해석할 수도 있다.

## Batch normalization(배치 정규화): 또다른 문제 + 해결 방법
잔차 연결로 그래디언트 문제를 해결했더니, 다른 문제가 터졌다. 레이어를 지날 때마다  
`입력`과 `F(입력)`이라는 두 신호가 더해지니, variance(분산)이 계속 두 배씩 커지는 문제. 이게 쌓이면 **활성화 값이 폭주**해버린다.

이 때 등장한 방법이 배치 정규화(Batch normalization, BN)이다.

### BN이 하는 일
1. 미니 배치 데이터에 대해 각 채널(또는 뉴런)의 평균과 표준편차를 구한다.
2. 이 값으로 데이터를 '강제 정규화'시켜서 평균 0, 표준편차 1로 만든다.
3. 그 다음, 네트워크가 학습 가능한 파라미터 감마(스케일)와 델타(이동)를 이용해 최적의 분포로 다시 살짝 조정해줌.

잔차 블록 시작 부분에 BN을 넣어주면, 매번 활성화 값의 스케일을 적절하게 조절해줘서 값이 폭주하는 걸 막고, 결과적으로 훨씬 깊은 네트워크를 안정적으로 학습시킬 수 있게 된다. 게다가 학습 속도를 높이고, 일종의 regularizaiton 효과까지 덤으로 얻을 수 있다.

## 이 아이디어로 만든 엄청난 모델들
- ResNet: 이 모든 아이디어의 집약체.
`BN > ReLU > Conv`로 구성된 잔차 블록을 사용. 특히 파라미터 효율을 높인 bottleneck 블록은 1x1 Conv로 채널을 줄였다가 > 3x3 Conv로 연산 > 다시 1x1 Conv로 채널을 늘리는 구조로 매우 깊은 네트워크를 가능하게 했다.
- DenseNet: ResNet의 덧셈(+)에서 한발 더 나아가 결합(concatenation)을 선택한 모델. 각 레이어의 출력을 다음 레이어에 더하는 게 아니라, 채널 방향으로 계속 이어 붙인다. 모든 과거의 정보를 끌어안고 가는 스타일이라 feature 재사용을 극대화했다.
- U-Net: 잔차 연결 아이디어를 인코더-디코더 구조에 적용한 대박 모델. 이미지 분할(segmentation)에서 엄청난 성공을 거둠. 인코더(이미지 압축)의 각 단계에서 나온 풍부한 공간 정보를 스킵 연결을 통해 디코더(이미지 복원)의 해당 단계에 그대로 전달해준다. 덕분에 압축 과정에서 손실될 수 있는 디테일한 정보를 되살려 정교한 결과물을 만들 수 있다.

## 정리
- 단순히 깊게 쌓으면 망한다(shattered gradients 현상 때문에 학습 불가)
- Skip connection은 그래디언트 고속도로를 뚫어 아주 깊은 네트워크의 학습을 가능하게 한 혁신임
- Batch Norm은 잔차 네트워크의 활성화 값 폭주를 막아주는 단짝이며, Resnet, DenseNet, U-net은 이 아이디어들을 바탕으로 만들어진 필수 교양 아키텍처이다.

1. 문제: VGG와 같은 CNN 모델을 19층보다 더 깊게 쌓았더니 오히려 성능이 떨어지는 문제 발생
2. 해결책: residual blocks(잔차 블록), 즉 스킵 연결(skip connection)이라는 개념을 도입. 이는 각 레이어가 출력을 완전히 새로 계산하는 대신, 입력에 대한 추가적인 변화량(additive change)만 학습하도록 하는 방식
3. 추가 해결책: 잔차 연결은 또 다른 문제(초기화시 활성화 값의 폭발적 증가)를 일으키는데, 이를 보완하기 위해 BN(배치 정규화)를 사용
4. 주요 아키텍처: 이러한 아이디어를 바탕으로 한 ResNet, DenseNet, U-Net과 같은 유명한 모델들이 있음
