---
layout  : wiki
title   : ch4 - deep neural networks(심층 신경망)
summary : 
date    : 2025-05-30 16:59:57 +0900
updated : 2025-05-30 22:34:10 +0900
tag     : dnn deep learning
resource: 33/2AAEA8-3206-47CC-B155-DDA8442C1C75
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
---
* TOC
{:toc}

## jargons


| 개념                                 | 설명                                                                                                                |
| ----------------                     | -----------------------------                                                                                       |
| deep neural networks(심층 신경망)    | 하나 이상의 은닉층(hidden layer)를 가진 신경망                                                                      |
| shallow neural networks(얕은 신경망) | 단 하나의 은닉층을 가진 신경망                                                                                      |
| width(너비)                          | 신경망 각 은닉층에 있는 hidden unit의 수를 의미                                                                     |
| depth(깊이)                          | 신경망에 있는 은닉층의 수                                                                                           |
| capacity(용량)                       | 신경망의 전체 hidden unit 수(모델의 복잡성이나 표현력을 나타내는 척도 중 하나)                                      |
| hyperparameters(하이퍼파라미터)      | 모델 학습 전에 사용자가 미리 설정하는 값들. 신경망에서는 은닉층의 수(깊이)나 각 층의 은닉 유닛 수(너비)등이 해당 됨 |
| over-parameterized deep models       | 훈련 데이터예시보다 더 많은 매개변수를 가진 심층 학습 모델                                                          |



## Deep: 합성 구조
- deep neural network = 여러 개의 shallow network 합성
    - \( f(x) = f3(f2(f1(x))) \)
- 각 레이어가 입력을 변형하고, 다음 레이어는 그 위에 연산을 덧붙임
- 이 구조 덕분에 입력 공간을 반복적으로 **구부리고 접는(folding)** 효과가 생김

### 신경망에서 "접기"가 일어나는 방식
- 주로 첫 번째 계층에서 이런 "접기" 역할을 수행함
- 활성화 함수(ReLU 같은)와 가중치의 조합을 통해, 입력 공간의 특정 패턴이나 영역들을 감지하고, 이들을 유사한 중간 표현으로 변환함
- 예를 들어, 입력 x의 값들이 특정 임계값을 넘거나, 특정 범위에 속할 때 유사한 활성화 패턴을 보이도록 학습될 수 있다. 이게 반복되면 여러 입력 영역이 동일한 중간 출력 영역으로 매핑되는 "접힘" 현상이 나타나는 것
- 접기의 효과: 효율적인 표현 학습(입력 공간의 다양한 부분에 나타나는 유사한 패턴을 하나의 방식으로 처리할 수 있게 됨)

> 접는다는 비유적 표현. 핵심 의미는 **서로 다른 여러 입력값을 동일한 출력값 또는 동일한 범위의 값으로 매핑한다**는 것이다.
종이접기에 비유한다면,
1. 긴 종이띠: 왼쪽 끝 부분, 중간 부분, 오른쪽 끝 부분이 있음
2. 이 종이띠를 두 번 접어서 세 겹으로 만들면, 원래는 서로 다른 위치에 있떤 세 부분이 이제 같은 위치에 겹쳐짐
3. 이 때, 종이를 접는 행위가 첫 번째 네트워크의 역할에 해당. 겹쳐진 종이 위의 한 지점이 중간 표현 y에 해당

결론적으로, "접는다"는 것은 신경망이 입력의 복잡성과 다양성을 줄여서, 더 적은 종류의 중간 패턴으로 일반화하고, 이를 바탕으로 후속 처리를 효율적으로 수행하게 하는 메커니즘이다. 이를 통해 깊은 네트워크는 더 적은 유닛으로도 복잡한 함수를 표현할 수 있게 된다.




## Depth vs Width
- 너비(width)=hidden unit 수를 늘리는 것도 복잡도 향상엔 도움되지만
- 깊이(depth)=layer 개수를 늘리는 것이 더 효율적이다
    1. depth efficiency(얕은 신경망으로는 표현이 불가능하거나 매우 많은 뉴런이 필요)
        - depth 10짜리 네트워크로 표현 가능 but shallow하게 표현하려면 수천 개의 뉴런 필요
    2. 복잡도 폭발 (선형 구간 수)
        - 깊이를 늘리면 이 선형 구간 수가 지수적으로 증가


### Deep vs Shallow
- 얕은 신경망: 퍼즐 조각 10개로 그림 만들기
- 깊은 신경망: 같은 퍼즐 조각 10개지만, 겹쳐서 다른 모양도 만들 수 있음
- 깊은 신경망은 같은 자원으로 더 많은 그림(함수)를 만들 수 있음

![deep](https://i.imgur.com/jCGcMmU.png)

위 그림은 깊이의 힘을 수치적으로 시각화한 그래프다.

- 같은 수 파라미터(471개)
- shallow > 37개의 linear region
- deep > 161051개의 linear region
- 왜 딥러닝에서 deep neural network를 쓰는가? > 더 효율적으로 더 복잡한 패턴을 학습할 수 있기 때문
- shallow network를 충분히 크게 만들면 안되나? > 이론상 가능하지만, 파라미터 수가 비효율적으로 커짐
- 위 그래프가 의미하는 바는? > depth는 모델의 표현력을 극적으로 확장시키는 열쇠다!

> 깊이를 늘리는 것만으로도, 같은 자원으로 훨씬 더 복잡한 문제를 풀 수 있다. 이게 딥러닝이 shallow 모델보다 뛰어난 가장 핵심적인 이유
