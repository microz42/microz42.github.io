---
layout  : wiki
title   : ch6 - fitting model(모델 피팅)
summary : 
date    : 2025-06-04 16:33:28 +0900
updated : 2025-06-07 23:03:01 +0900
tag     : fitting learning training
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : true
resource: F17E62DB-0B2C-5645-5B72-4CC8BF6E6B60
---
* TOC
{:toc}

이번 장에서는 모델을 어떻게 학습(손실 함수 값을 최소로 만드는 모델 파라미터를 어떻게 찾는지)시키는지에 대한 내용이다.

핵심
1. 일단 파라미터 값을 대충 정한다(초기화)
2. 손실 함수 값이 작아지는 방향으로 파라미터를 찔끔찔끔 수정하는 걸 반복(gradient descent/경사 하강법)
	1. 현대 파라미터에서 기울기(gradient, 손실 함수를 각 파라미터로 미분한 값)을 계산. 이 기울기가 손실 함수 값이 가장 가파르게 증가하는 방향을 알려준다
	2. 기울기의 반대 방향 (가장 가파르게 감소하는 방향)으로 파라미터를 살짝 업데이트

위 단계들을 계손 반복하면 언젠가 손실 함수의 최저점에 도달하겠지... 하는 마음으로.


## Gradient Descent 경사 하강법
말 그대로 손실 함수라는 언덕을 기울기만 보고 한 걸음씩 내려가는 방법
1. 현재 파라미터(ϕ)에서 손실 함수L[ϕ]의 기울기를 계산
2. 파라미터를 업데이트
$$\phi \leftarrow \phi - \alpha \cdot \frac{\partial L}{\partial \phi}$$
	- α는 학습률(Learning Rate)
	- 너무 크면 최저점을 지나쳐 버리고, 너무 작으면 느리다
	- 기울기가 0이 되면 (평평한 곳에 도달하면) 더 이상 내려갈 곳이 없어 파라미터 업데이트가 멈춘다

### Convex(깔끔 볼록) vs. Non-convex(울퉁불퉁)
- 선형 회귀처럼 손실 함수가 예쁜 그릇 모양이면 (볼록 함수) 어디서 시작하든 경사 하강법은 무조건 global minimum으로 데려다준다
- 하지만 대부분의 딥러닝 모델 (비선형 모델)의 손실 함수는 울퉁불퉁 복잡하게 생겼다


## 울퉁불퉁한 손실 함수(골칫거리)
- Local Minimum(지역 최저점)
	- 주변보다 낮은 지점이라 기울기가 0
	- 하지만 진짜 최저점(전역 최저점)은 아닌 가짜다
	- 경사 하강법은 여기에 빠지면 헤어 나오질 못한다.
- Saddle Point(안장점)
	- 기울기는 0인데, 어떤 방향으로는 오르막이고 다른 방향으로는 내리막인 희한한 지점

모델 파라미터가 수백만 개씩 되는 딥러닝에서는 이런 지역 최저점이나 안장점이 훨씬 더 많고 복잡해서,  
단순 경사 하강법으로는 전역 최저점을 찾기가 하늘의 별따기다

---

## Stochastic Gradient Descent, SGD(확률적 경사 하강법)
단순 경사 하강법은 시작점에 따라 운명이 결정되는 반면,  
SGD는 여기에 랜덤성을 살짝 추가해서 이 문제를 해결한다.

**핵심**: 
- 전체 훈련 데이터를 한 방에 다 써서 기울기를 계산하는게 아니라 XXXX
- 데이터를 작은 묶음(mini-batch / batch)으로 나누고,
- 각 iteration마다 랜덤하게 한 묶음씩 뽑아서
- 그 묶음의 데이터만으로 기울기를 계산하고 업데이트 한다
	- epoch: 전체 훈련 데이터를 한 번 다 훑어보는 걸 1 에포크라고 함
	- 배치 크기는 1개부터 전체 데이터까지 다양하게 설정 가능
	- 전체 데이터를 쓰면 그냥 경사 하강법이랑 똑같아짐

### SGD 성능 이유
1. 왔다리갔다리 경로: 매번 다른 데이터 쪼가리만 보니까 기울기가 좀 들쑥날쑥하다. 그래서 평균적으로는 내리막길을 가지만, 가끔은 오르막기로 점프한다. (이래서 지역 최저점에서 탈출 가능)
![gd_vs_sgd](https://i.imgur.com/2wG5K2w.png)

2. 계산량: 전체 데이터 대신 일부만 쓰니까 한 번 업데이트할 때 계산이 훨씬 빠름
3. 안장점 탈출 용이: 안장점 근처에서도 다른 배치를 뽑으면 기울기가 0이 아닐 가능성이 높아서 덜 갇힘
4. 일반화 성능 향상 (카더라): SGD로 찾은 파라미터가 실제 새로운 데이터에서도 잘 작동하는 경향이 있다고 함(9장)

learning rate 스케줄링: SGD는 처음에 큰 학습률로 성큼성큼 탐색하다가, 어느 정도 최저점 근처에 가면 학습률을 줄여서 세밀하게 조정하는 Learning rate schedule을 사용한다.


## Momenum
모멘텀을 추가하면 마치 언덕 내려가는 공처럼 관성을 받아서 더 부드럽고 빠르게 최저점으로 갈 수 있다.

업데이트 방식: 현재 배치의 기울기뿐만 아니라, **이전 업데이트 방향**을 일정 비율(β)로 함께 고려해서 다음 업데이트 방향을 정한다
- 기울기 방향이 계속 비슷하면 가속도가 붙고, 왔다 갔다 하면 관성이 상쇄됨

Nesterov Accelerated Momentum, NAG:
기존 모멘텀이 현재 위치에서 기울기를 보고 "일단 가고 보자"였는데, 네스테로프는 "일단 관성으로 좀 가보고, 거기서 기울기를 ㅖ산해서 방향을 수정하자" 느낌.

## Adam(Adaptive Moment Estimation): 학습률 조절기
경사 하강법의 문제 중 하나. 기울기가 큰 방향으로는 너무 많이 가고 작은 방향으로는 찔끔 가서 학습률 정하기가 까다롭다는 점.

Adam 아이디어:
1. 기울기 정규화
	- 각 파라미터 방향으로 이동하는 거리를 고정(기울기 부호만 보고 학습률만큼 이동)
	- 근데 이건 최저점에서 왔다갔다 진동하는 문제가...
2. 모멘텀 활용
	- 그래서 Adam은 기울기 자체의 이동 평균(모멘텀처럼)과
	- 기울기 제곱값의 이동 평균을 함께 사용한다
		- 기울기 제곱값의 평균은 각 파라미터별로 학습률 크기를 조절하는 역할을 한다(기울기 변화가 심하면 학습률 작게, 변화가 적으면 크게)
3. 초기 보정
	- 학습 초반에는 이동 평균 값들이 너무 작게 추정되는 걸 막기 위해 보정 항을 사용해.
4. 최종 업데이트
	- 보정된 기울기 평균을 보정된 기울기 제곱 평균의 제곱근으로 나눠서 업데이트

Adam의 장점
- 각 파라미터마다 알아서 학습률을 조절해 줘서, 방향에 따라 기울기가 심하게 다른 경우에도 잘 찾아감
- 학습률 초기값에 덜 민감하고, 복잡한 학습률 스케줄이 덜 필요함
- 보통 SGD와 함께 미니배치를 사용
- 신경망 깊이에 따라 기울기 크기가 달라지는 것도 어느 정도 보완해 줘

---

## 하이퍼파라미터 튜닝 = 짬바가 있어야 한다
어떤 학습 알고리즘을 쓸지 (SGD, Adam 등),  
배치 크기는 얼마로 할지  
학습률은 어떻게 정할지,  
모멘텀 계수는 뭘로 할지...  

이런 것들은 모델 파라미터랑은 별개로 직접 정해줘야 하는 하이퍼파라미터다.
- 모델 파라미터: 모델이 스스로 학습하는 값(가중치, 편향)
- 하이퍼파라미터: 모델을 학습시키기 위해 사람이 직접 설정해주는 값(학습률, 배치 크기)
- 훈련 = 이 파라미터들을 잘 조절해서 모델이 주어진 데이터에 대해 최상의 예측을 하도록 만드는 과정

이걸 잘 정하는 건 약간 예술의 영역이라...  
보통 여러 조합으로 모델을 돌려보고 제일 잘 나오는 걸 고르곤 함


## 정리
- 경사 하강법: 기울기 따라 내려가기
- 울퉁불퉁 손실 함수: 지역 최저점, 안장점 조심
- SGD: 랜덤 미니배치로 노이즈 추가해서 탈출 시도 + 계산 효율 UP
- 모멘텀: 관성으로 더 빠르고 부드럽게
- Adam: 똑똑하게 파라미터별 학습률 조절


