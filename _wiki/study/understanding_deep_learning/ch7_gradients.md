---
layout  : wiki
title   : ch7 - 역전파와 초기화의 중요성
summary : 
date    : 2025-06-07 22:50:05 +0900
updated : 2025-06-11 22:24:22 +0900
tag     : backpropagation
resource: 2F/6AD52E-84D3-4E60-B180-D75C80A5FD5F
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
---
* TOC
{:toc}

이번 챕터에서의 핵심은 딱 두가지
1. 경사하강법 (Gradient Descent)의 재료인 'Gradient'를 어떻게 효율적으로 구할 것인가?
2. 훈련 시작점, 즉 파라미터 초기값을 어떻게 잘 설정해서 삽질을 피할 것인가?

## Backpropagation: 똑똑하게 Gradient 계산하기
한마디로, 모델을 똑똑하게 만드는 효율적인 방법이라고도 할 수 있는데,  

- 학습이란?
    - 모델에 파라미터(가중치, 편향)가 수억개씩 있는데, 얘네들을 조금씩 정답을 더 잘 맞히게 만들기

그리고 이 학습이란 걸 할 때  
**어느 방향으로 얼마나 값을 바꿔야 정답에 가까워지는지** 알려주는 신호가 기울기(gradient).  
SGD 같은 옵티마이저는 이 기울기 값을 보고 파라미터를 업데이트하는 거고.

여기서 문제는 이 수많은 파라미터에 대한 기울기를 어떻게 계산하느냐?인데

역전파는 바로 이 기울기를 가성비로 계산해주는 고효율 알고리즘.

### 역전파 2단계 핵심 프로세스
1. Forward Pass
2. Backward pass

#### Forward Pass: 일단 끝까지 달리기(기록 하면서)
일단 입력 데이터 `x`를 모델에 넣고, 첫번째 레이어부터 마지막 레이어까지 쭉 계산해서 예측값과 Loss를 구한다.  

여기서 중요한 점
- 단순히 결과만 보는게 아니라!
- 계산 과정에서 나온 모든 중간값(각 레이어의 활성화 값 `h`와 그 직전값 `f`)을 전부 메모리에 저장해둬야 함  
    - 게임 세이브 포인트 만든다고 생각하면 쉬움
    - 이 세이브 포인트를 backward pass에서 사용

#### Backward Pass: 범인 이 안에 있다(거꾸로 추적)
최종적으로 나온 Loss 값에서부터 시작해서, 계산 과정을 거꾸로 한 단계씩 되짚어 올라간다.

**그래서 loss에 얼마나 영향을 미쳤는지?**를 알아내는 과정
- **마지막 레이어부터 시작**
    - "최종 손실아, 너 바로 전 단계인 출력층이 1만큼 바뀌면 넌 얼만큼 바뀌냐?" 이걸 계산
- 한 단계씩 앞으로: "출력층아, 너 바로 전 단계인 3번 은닉층이 1만큼 바뀌면 넌 얼만큼 바뀌냐?"

이런식으로 계속 거슬러 올라간다.  
이때 Chain Rule이 사용된다
- Chain Rule:
    - 쉽게 생각해서 "뒷 단계의 영향력 X 현재 단계의 영향력"을 계속 곱해나가는 거

거꾸로 계산의 이점 = **계산 재활용**  
2번 은닉층의 기울기를 계산할 때, 위에서 계산했던 3번 은닉층의 기울기 값을 그대로 가져와서 쓸 수 있다.  
덕분에 중복 계산이 사라지고 속도가 미친 듯이 빨라진다.

이렇게 각 레이어가 최종 손실에 미치는 영향을 거꾸로 다 전파하고 나면, 최종적으로 원했던 각 파라미터가 손실에 얼마나 영향을 미치는지(기울기)를 알 수 있게 된다.


## Parameter Initiazliation: 시작이 반
역전파로 기울기 구하는 법은 알았는데, 학습을 시작하려면 파라미터들이 맨 처음에 어떤 값을 가져야 하는가?

이게 바로 initialization(초기화) 문제다.

만약 초기 가중치 값을 너무 작게 잡으면?
- Vanishing Gradients(기울기 사라짐)
    - 레이어를 지날수록 신호가 점점 약해져서 거의 0이 되버린다.
    - 역전파도 기울기가 뒤로 갈수록 사라져서 앞쪽 레이어들은 거의 학습이 안되는 참사 발생

반대로 너무 크게 잡으면?
- Exploding Gradients(기울기 폭주)
    - 신호가 레이어를 지날수록 눈덩이처럼 커져서 결국엔 무한대(inf)나 NaN(Not a Number)가 떠버린다.
    - 모델이 그냥 터짐

### 해결책: He initialization
똑똑한 사람들이 연구해보니,  
**"각 레이어의 출력값 분산(데이터가 퍼진 정도)을 입력값의 분산과 비슷하게 유지해주면" 이 문제가 해결된다는 걸 알아냄 (pg 110 복습)

특히 자주 쓰이는 ReLU 활성화 함수를 쓸 때는,  
- 가중치를 평균 0
- 분산 2/Din (Din은 입력 노드 수)
인 정규분포에서 뽑아서 초기화하면 된다고 한다.

이걸 He 초기화라고 부른다.  
이렇게 하면 학습이 안정적으로 시작될 수 있다

## 정리
1. 역전파는 딥러닝 모델의 수많은 파라미터 기울기를 효율적으로 계산하는 알고리즘이다
2. Forward Pass로 중간 계산 값을 모두 저장하고, Backward Pass로 손실부터 거꾸로 추적하며 계산을 재활용하는게 핵심
3. 학습 안정성을 위해 가중치 초기화가 중요하며, He 초기화 같은 기법으로 기울기 소실/폭주 문제를 방지할 수 있다.

이 모든게 `loss.backward()` 호출할 때 벌어지는 일...

