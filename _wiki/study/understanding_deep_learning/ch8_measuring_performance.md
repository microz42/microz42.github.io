---
layout  : wiki
title   : ch8 - 성능 측정
summary : 
date    : 2025-06-20 14:20:50 +0900
updated : 2025-06-20 14:47:10 +0900
tag     : 
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
resource: FAB97A76-1E87-671C-28A9-61FD6830C714
---
* TOC
{:toc}
모델을 잘 만들었는지, 진짜 실력을 평가할 차례

## 핵심
1. train 잘 되었나? > Train/valida/test 데이터 세트 나누기
2. test 에러는 왜 생기는지? > bias(편향), variance(분산), noise(노이즈)
3. 모델을 키울까, 줄일까? > Bias-Variance Trade-off
4. 모델 클수록 성능은 좋아짐 > Double descent(이중 강하)

## 데이터 3등분하기
코드를 돌리면 훈련 에러(training error)는 쭉쭉 내려가서 0%에 가까워진다. 하지만 이건 모델이 훈련 데이터를 *통째로 외워버린* 것일 수 있다. **일반화(generalization) 성능(새로운 데이터에 대한 성능)**이 중요하다.

그래서 데이터를 아래처럼 3가지로 나눌 수 있다.
- training set: 모델의 weight(가중치)와 bias(편향)을 학습시키기 위해 사용
- validation set: 하이퍼파라미터(learning rate, 레이어 개수, 히든 유닛 개수 같은)를 튜닝하기 위해 사용. 여러 하이퍼파라미터 조합으로 모델을 훈련시킨 뒤, 검증 세트에서 가낭 성능이 좋은 걸로 고른다.
- test set: 모든 튜닝이 끝난 최종 모델의 성능을 딱 한 번 평가하기 위해 사용한다.

>[!Warning]
> 테스트 세트로 하이퍼파라미터를 튜닝하는 건 금지!
> 그건 족보를 보고 공부하는 것과 같아서, 진짜 실력을 측정할 수 없음

## 테스트 에러의 3가지 원흉: 노이즈, 편향, 분산
모델의 테스트 에러는 3가지 요소의 합으로 볼 수 있다.
[!test_error](https://i.imgur.com/tVrdG4g.png)

1. Noise
	- 데이터 자체에 내재된 어쩔 수 없는 무작위성이나 오류
	- 예를 들어, 같은 사진이라도 라벨링이 잘못되었거나, 관찰되지 않은 변수가 영향을 미침
	- 이건 통제 불가한 영역이라 모델이 아무리 좋아도 줄일 수 없는 에러의 하한선과도 같음
2. Bias
	- 모델이 너무 단순해서 데이터의 복잡한 패턴을 제대로 표현하지 못할 때 발생하는 에러
	- 편향이 높은 모델 == underfit 되기 쉬움
	- 모델의 capacity를 늘려서(레이어나 유닛 추가) 편향을 줄일 수 있다
3. Variance
	- 모델이 훈련 데이터의 노이즈까지 너무 민감하게 학습해서 발생하는 에러
	- 훈련 데이터가 조금만 바뀌어도 모델이 크게 달라진다
	- 분산이 높은 모델 == overfit 되기 쉬움
	- 훈련 데이터를 더 많이 확보해 분산을 줄일 수 있다

## 편향-분산 트레이드오프
- 단순한 모델(Low capacity): 데이터 패턴을 잘 못 잡아서 **편향은 높고**, 데이터 노이즈에 둔감해서 **분산은 낮다**
- 복잡한 모델(high capacity): 데이터 패턴을 잘 잡아내서 **편향은 낮지만**, 노이즈까지 외워버려서 **분산이 높다**
따라서 모델의 복잡도를 조절하며 편향과 분산의 합이 최소가 되는 '최적점(sweet spot)'을 찾아야 한다.

## Double Descent(이중 강하)
모델 용량을 계속 늘리다 보니, 과대적합으로 테스트 에러가 정점을 찍고 다시 **감소하기 시작**하는 현상을 발견할 수 있었는데, 이걸 **이중 강하**라고 부른다.

- under-prameterized: 모델 파라미터 수가 훈련 데이터 수보다 적은 구간. 여기서는 편향-분산 트레이드오프가 나타남
- over-parameterized: 모델 파라미터 수가 훈련 데이터 수보다 많은 구간. 이 구간에서는 모델이 클수록 오히려 성능이 좋아진다.

### 왜????
완벽히 규명된 건 아니지만, 유력한 설명으로는
- 모델의 용량이 훈련 데이터를 겨우 외울 만큼만 있을 때 (에러 피크 지점), 모델은 데이터를 억지로 통과하기 위해 매우 삐뚤빼둘하고 불안정한 함수가 된다
- 하지만 용량이 훨씬 더 커지면(over-parameterized), 모델은 훈련 데이터를 모두 통과하는 수많은 함수 중에서도 가장 부드러운(smoothest) 함수를 선택할 여유가 생김
- 고차원 공간에서 데이터는 매우 희소하기 때문에(차원의 저주), 데이터 포인트 사이를 부드럽게 보간(interpolate)하는 것이 새로운 데이터에 대해 더 잘 일반화될 가능성이 높다.

>[!memo]
> Double Descent 현상은 "모델이 너무 크면 무조건 과대적합되니 안 좋다"는 고정관념을 깨준다. 아주 큰 모델을 쓰는 것이 더 좋은 일반화 성능으로 이어질 수 있다는 걸 시사.

## 정리
1. 데이터는 꼭 Train/validation/test로 나눠서 쓰자. 성능을 정직하게 측정하는 첫걸음
2. 에러의 원인을 파악하자. 모델 성능이 안 좋다면 편향 문제)몬델이 너무 단순)인지, 분산 문제(모델이 너무 복잡/데이터 부족)인지 고민해야 한다
3. 편향-분산 트레이드오프는 여전히 유효한 개념이다. 하지만 딥러닝에서는 이게 끝이 아닐 수 있다.
4. Double Descent! 때로는 파라미터가 데이터보다 훨씬 많은 '오버-파라미터' 모델이 더 좋은 성능을 낼 수 있다. 따라서 모델 크기를 결정할 때 너무 겁먹을 필요는 없다. 최종 선택은 언제나 검증 세트의 성능이 말해줄 것이다.
