---
layout  : wiki
title   : ch9 - 정규화
summary : 일반화 성능 끌어올리는 비법
date    : 2025-06-20 14:53:55 +0900
updated : 2025-06-20 15:38:02 +0900
tag     : regularization deeplearning generalization 일반화
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
resource: 39E991E0-A401-3F85-86E6-2E1943FEFF96
---
* TOC
{:toc}

훈련 에러는 0인데 테스트 에러는 폭망하는 상황 막아줄 치트키

## 정규화(Regularization)란?
훈련 데이터와 테스트 데이터 사이의 성능 격차를 줄여주는 모든 방법을 통칭하는 말. 모델이 훈련 데이터의 지엽적인 특징에만 과대적합 되는 것을 막아 일반화 성능을 높이는 것이 목표로 크게 3가지로 나눌 수 있다.
1. 명시적 정규화(explicit regularization): 손실 함수에 대놓고 페널티 항을 추가하는 방법
2. 암시적 정규화(implicit regularization): 나도 모르는 사이, SGD 같은 최적화 알고리즘이 특정 해를 선호하게 되는 현상
3. 각종 휴리스틱(heuristics): 경험적으로 일반화에 효과가 있다고 알려진 다양한 실전 기법들

### 명시적 정규화: L2 정규화(가중치 감쇠)
L2 정규화는 가장 대표적인 정규화 방법이다. 손실 함수에 가중치들의 제곱 합에 비례하는 페널티 항을 추가하는 방식.
- 작동 원리: 가중치가 너무 커지는 것을 막아 더 작은 가중치를 갖도록 유도
- 효과: 가중치가 작아지면 모델이 만들어내느 함수가 더 부드러워진다. 부드러운 함수는 데이터의 큰 흐름을 학습하게 되므로, 노이즈에 과대적합될 가능성이 줄어들고 데이터가 없는 영역에서도 더 안정적인 예측을 할 수 있다.

[!L2](https://i.imgur.com/ONlpePx.png)
- λ값이 커질수록(정규화 강도가 세질수록), 모델(청록색 선)이 데이터 포인트를 덜 따라가고 부드러워지는 걸 볼 수 있다.


### 2. 암시적 정규화: SGD 숨겨진 능력
최근의 흥미로운 발견 중 하나는, 우리가 쓰는 최적화 알고리즘 자체가 특정 해를 선호하는 '편향'을 가진다는 것이다.
- SGD는 각 미니배치의 그래디언트가 서로 비슷한, 즉 그래디언트의 분산이 낮은 영역의 해를 암묵적으로 선호한다.
- 왜? : 모든 데이터에 대해 일관된 방향으로 손실을 줄이는 해를 찾도록 유도하기 때문에, 특정 데이터에만 과하게 맞춰진 해보다 일반화 성능이 더 좋을 가능성이 높다. 이것이 작은 배치 사이즈가 종종 더 좋은 성능을 내는 이유 중 하나로 꼽힌다.


### 3. 실전에 사용되는 각종 정규화 휴리스틱
이론적인 배경 외에도, 실전에서 성능을 올리기 위해 사용하는 다양한 기법들이 존재한다

[!early_stop](https://i.imgur.com/B84MFi3.png)
- early stopping(조기 종료)
	- 모델이 훈련 데이터의 노이즈까지 학습하기 전, 즉 과대적합이 시작되기 전에 훈련을 멈추는 기법
	- 검증 세트의 성능이 가장 좋았던 시점의 모델을 최종 모델로 선택

[!ensemble](https://i.imgur.com/vwOR8wd.png)
- ensembling(앙상블)
	- 여러개의 모델을 독립적으로 학습시킨 뒤, 그 예측을 평균 내거나 투표해 최종 예측을 결정하는 방법
	- 개별 모델의 오류가 서로 상쇄되어 더 안정적이로 정확한 예측 가능

[!dropout](https://i.imgur.com/4N01hE1.png)
- dropout(드롭아웃)
	- 훈련시 각 스텝마다 은닉층의 뉴런 중 일부를 랜덤하게 비활서오하(출력을 0으로 만듬)하는 기법
	- 모델이 특정 뉴런에 과도하게 의존하는 것을 막고, 결과적으로 더 강건한 특징(feature)을 학습하게 함
	- 테스트 시에는 모든 뉴런을 사용하되, 드롭아웃 비율만큼 가중치를 줄여서 균형을 맞춘다(weight scaling)

[!noise](https://i.imgur.com/aWgdhfp.png)
- 노이즈 추가 & 레이블 스무딩
	- 훈련 과정에서 입력 데이터, 가중치, 또는 레이블에 의도적으로 노이즈를 추가해 모델을 더 강건하게 만듬
	- 특히 레이블 스무딩은 정답 레이블을 [0, 0, 1, 0]처럼 이분법적인 형태(확신에 찬(?) 형태) 대신 [0.025, 0.025, 0.9, 0.025]처럼 부드럽게 만들어 모델의 과신을 방지하고 일반화 성능을 높인다
- 데이터 활용

[!data_augmentation](https://i.imgur.com/SYYANH9.png)
	- data augmentation(데이터 증강)
		- 이미지 분류 문제에서 이미지를 회전시키거나 뒤집어도 여전히 같은 레이블을 갖는 것처럼, 데이터의 본질을 바꾸지 않는 선에서 변형을 가해 훈련 데이터를 늘리는 방법
	
	
[!transfer_self_supervised](https://i.imgur.com/Yvt67k7.png)
	- transfer learning(전이 학습)
		- 데이터가 풍부한 큰 데이터셋으로 미리 훈련된 모델(pre-trained model)을 가져와, 내가 풀고 싶은 문제에 맞게 마지막 레이어 등을 조금 수정해 다시 훈련(fine-tuning)하는 기법
		- 적은 데이터로도 좋은 성능을 낼 수 있는 강력한 방법
	- self-supervised learning(자기 주도 학습)
		- 레이블이 없는 대규모 데이터에서 스스로 문제들 만들고 풀게 해(예: 이미지의 일부를 가리고 맞추게 하기) 유용한 표현(representation)을 학습한 뒤, 이를 전이 학습에 활용하는 기법

## 정리
- L2 정규화(가중치 감쇠)는 과대적합을 막는 가장 기본적인 방법(모델이 만드는 함수를 부드럽게 만들어줌)
- SGD와 작은 배치 사이즈는 그 자체로 암시적인 정규화 효과를 가짐(일반화에 유리)
- 조기 종료, 드롭아웃, 앙상블은 효과가 입증된 정규화 기법들
- 데이터가 왕. 데이터가 부족하면 데이터 증강을 적극 활용하고, 전이 학습을 통해 이미 잘 학습된 모델의 지식을 빌려오는 것을 최우선으로 고려하자.
