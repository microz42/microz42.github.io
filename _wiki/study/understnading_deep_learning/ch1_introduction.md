---
layout  : wiki
title   : ch1 - Introduction
summary : 책 개요
date    : 2025-05-14 14:53:03 +0900
updated : 2025-05-14 15:49:14 +0900
tag     : study
resource: 6A/053122-5C3B-427E-9488-21336C60E776
toc     : true
public  : true
parent  : [[/study/understanding_deep_learning]]
latex   : false
---
* TOC
{:toc}

## Machien Learning vs Deep Learning
머신러닝은 인공지능의 한 분야로, **관찰된 데이터에 수학적 모델을 적용해 결정을 내리는 방법을 학습**한다. DNN(Deep neural network, 심층 신경망)은 이러한 머신러닝 모델의 한 종류이다. 이런 모델을 데이터에 맞추는 과정을 **딥러닝**이라고 부른다.

머신러닝은 크게 세 가지 영역으로 나뉜다:
1. supervised learning(지도 학습)
2. unsupervised learning(비지도 학습)
3. reinforcement learning(강화 학습)

## Supervised learning
지도 학습은 **입력 데이터에서 출력 값으로의 매핑**을 정의한다.  
쉽게 말하면,  
입력에 대한 정답이 있는 데이터를 통해 모델이 학습하는 방식이다.

### regression(회귀)와 classification(분류)
실제 세계의 입력은(글, 사진, 음성 파일 등)은 숫자 벡터로 인코딩되어 모델에 투입된다.  
모델은 이 입력 벡터를 받아 출력 벡터를 반환하는 **블랙박스**처럼 작동한다

#### Regression(회귀)
- 모델이 **연속적인 숫자**를 반환(범주 할당X)
- **multivariate(다변량 회귀)**는 하나 이상의 숫자를 예측한다

#### Binary classification(이진 분류)
- 모델이 입력을 **두 개의 범주 중 하나**에 할당
- 출력 벡터는 입력이 각 범주에 속할 확률을 포함

#### Multiclass classification(다중 분류)
- 모델이 입력을 ** N > 2**개의 범주 중 하나에 할당
- 모델은 N개의 범주에 대한 확률을 포함하는 크기 N의 벡터를 반환

### 머신러닝 모델
머신러닝 모델은 입력과 출력 사이의 관계를 설명하는 수학적 방정식과 같다.  
예) 나이에 따른 평균 키가 어떻게 변하는지 설명하는 방성식

모델을 **학습**하거나 **적합**시킨다는 것은 입력에서 출력으로의 관계를 가장 정확하게 설명하는 방정식을 찾는 것을 의미한다.  
이를 위해서 **label(정답)이 있는 입력/출력 쌍**이 필요하다

## Unsupervised Learning
비지도 학습은 **label 없이** 입력 데이터만으로 모델을 학습시킨다.  
입력에서 출력으로의 매핑을 학습하는 대신, 데이터의 구조를 이해하거나 설명하는 것이 목표다.

### Generative Models(생성 모델)
생성적 비지도 학습 모델은 **훈련 데이터와 통계적으로 구별할 수 없는 새로운 데이터 예시를 합성**하는 방법을 학습한다.  
또한 일부 출력이 미리 결정된 제약 조건 하에서 데이터를 합성할 수도 있다(조건부 생성이라고 함)  
예) image inpainting, text completion  

> 하지만 실제로 모델은 언어의 통계에 대해서만 알고 있을 뿐, 자신의 답변의 의미를 이해하지는 못한다.

### Latent variables(잠재 변수)
- 현실 세계의 데이터는 고차원처럼 보이지만, 실제로는 **저차원 구조**를 가짐
- 예: 영어 문장은 무작위 단어 조합보다 훨씬 적은 수
- 딥러닝은 이 ** 잠재 변수 ↔ 실제 데이터** 사이의 매핑을 학습한다

예를 들어, 텍스트 -> 이미지 생성 문제를 직접 해결하는 대신  
**텍스트의 잠재 변수 ↔ 이미지의 잠재 변수**를 연결하는 적븐이 더 효율적이다
- 데이터 쌍이 적어도 학습 가능
- 더 **그럴듯한 결과물** 생성 가능
- 다양한 결과 생성 가능 (랜덤성 도입)

## Reinforcement Learning
agent가 reward(보상)을 최대화하는 방향으로 행동을 학습
- 시간이 지난 후에 보상이 주어질 수 있음 > temporal credit assignment problem(시간차 보상 문제)
- 탐험(exploration)과 활용(exploitation) 사이의 균형 필요

예시:
- 체스: 말 하나씩 딸 때마다 보상, 혹은 게임 승리 시 단일 보상(단일 보상의 경우 시간차 보상 문제가 심해짐)
- 전략: policy network(정책 네트워크)로 관측된 상태 > 행동 결정


## AI 윤리적 고려사항
- 편향과 공정성: 학습 데이터의 편향 반영
- 설명 가능성: 모델의 결정 과정을 이해하기 어려움
- AI의 무기화: 기술이 해로운 방식으로 사용
- 권력 집중: 특정 기관이나 개인에게 집중되는 과도한 권력
- 존재 위험: 인류에게 위험 초래하는 잘못된 AI 개발
- [AI 윤리 무료 강의 - 헬싱키 대학](https://ethics-of-ai.mooc.fi/)


## 책 구성
- 2 - 9장: 지도 학습
- 10 - 13장: DNN 구조(CN, transformer emd)
- 14 - 18장: 비지도 학습
- 19장: 강화 학습

> Mathematics, you see, is not a spectator sport.



