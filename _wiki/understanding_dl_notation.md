---
layout  : wiki
title   : 딥러닝 수식 기호 해석
summary : UDL 읽으면서 나오는 기호
date    : 2025-06-02 11:50:27 +0900
updated : 2025-06-02 14:22:17 +0900
tag     : notation symbol deeplearning ml math
resource: A6/8204AF-768C-4829-BC7D-04C25DE3F934
toc     : true
public  : true
parent  : [[index]]
latex   : false
---
* TOC
{:toc}

문서에서 자주 등장하는 수학 기호 정리

## 주요 수식 기호

| 기호                            | 이름                                  | 설명                                                                                                              | 예시                  |
| ---                             | ---                                   | ---                                                                                                               | ---                   |
| (𝑓∘𝑔)                           | composite function/합성함수           | 중첩된 함수 <br> 세탁기(g) 돌리고 > 건조기(f) 돌리기<br>모델이 여러 층을 거치며 데이터를 처리하는 방식과 동일하다 | (f·g)(x) = f(g(x))    |
| ∆                               | delta/델타                            | 변화량, 얼마나 바뀌었냐  <br> ∆weight = 100kg - 90kg = 10kg 감소                                                  | ∆x = x₁ - x₀          |
| e                               | Euler's number/오일러 상수(≈2.718)    | 로지스틱 함수 같은 데서 나옴<br> 확률 뽑을 때 자주 등장                                                           | s = 1 / (1 + e⁻ᶻ)     |
| ∑                               | sigma/시그마                          | 전부 더하기<br>벡터나 미니배치 처리할 때 자주 씀                                                                  | ∑ xᵢ = x₁ + x₂ + x₃ … |
| ∏                               | Product(Pi)/파이                      | 곱하기 버전 시그마. 전부 곱해!<br> 확률 계산 같은 데서 자주 나옴                                                  | ∏ xᵢ = x₁·x₂·x₃ …     |
| 𝜖                               | Epsilon/엡실론                        | 거의 0에 가까운 수<br>수치 안정화용 또는 "0으로 나누기 방지용 보험" 느낌                                          | lr = 1e-4             |
| φ                               | phi / 파이(피)                        | 파라미터 함수<br>특성 변환 함수 또는 모델 내부 구조                                                               |                       |
| φ̂                               | phi hat /파이 햇                      | φ가 파라미터 함수, φ̂는 훈련 데이터로 학습된 함수<br>hat(^) 표기는 추정량을 뜻할 때 흔하게 사용됨                  |                       |
| Pr(y₁, ..., yᵢ  \| x₁, ..., xᵢ) | probability of y given x(조건부 확률) | 입력 x들이 주어졌을 때, y들이 나올 확률                                                                           |                       |
| θ                               | theta/쎄타                            | 모델이 학습하는 파라미터 전체 (weights, bias 등)<br>경사하강법으로 계속 업데이트됨                                |                       |

## 통계 관련


| 기호 | 이름                                      | 설명                                                      | 예시                        |
| ---  | ---                                       | ---                                                       | ---                         |
| μ    | mu(population mean)/뮤(모평균)            | 전체 데이터(모집단)의 평균<br>모집단의 진짜 평균          |                             |
| 𝑥̄    | x bar(sample mean)/표본평균               | 샘플 평균<br>일부 샘픙 평균, 데이터 일부만 보고 구한 평균 |                             |
| σ²   | sigma squared(population variance)/모분산 | 전체 데이터가 평균에서 얼마나 퍼졌나 측정                 | 클수록 데이터 뿔뿔이 흩어짐 |
| 𝑠²   | s squared(sample variance)/표본분산       | 샘플만 보고 계산한 분산(보정 포함)                        |                             |
| σ    | sigma(std dev)/표준편차                   | 분산의 루트                                               | σ = sqrt(σ²)                |
| 𝑠    | s(sample std dev)/표본 표준편차           | 표본 기반 편차                                            | s = sqrt(s²)                |
| ρₓ   | rho(x)/상관계수(correlation)              | X와 Y가 얼마나 같이 움직이냐                              | p=1이면 완전 정비례         |
| 𝑥̃    | x tilde(median)/중앙값                    | 정렬했을 때 딱 가운데 있는 값                             | 평균보다 극단값에 덜 민감   |



## Tip
- ∑랑 ∏는 각각 for 루프에서 sum이냐 product냐 차이
- 𝜖은 대부분 수치 안정성용. 그냥 공식에 박아두고 무시하면 편함
- (f ∘ g)는 모델 여러 층 쌓은 거랑 똑같다고 보면 됨. f(g(x)) = Layer2(Layer1(input))
- μ vs 𝑥̄
    - 진짜 전체 평균 vs 샘플링해서 계산한 추정 평균
- σ² vs 𝑠²
    - 진짜 분산 vs 샘플 분산 (n-1 보정 들어감)
- σ vs 𝑠
    - 분산의 제곱근 → “흩어진 정도”를 원래 단위로 본 거
- ρₓ
    - X랑 Y가 얼마나 같이 움직이는지
- 𝑥̃
    - 정렬 기준 중간값. 이상치에 안 휘둘리는 튼튼한 값!
- θ는 조미료: 요리사(optimizer)가 손맛(gradient descent)으로 계속 조절하면서 맛을 맞추는 재료
    - θ는 모델이 배우는 대상 (weight, bias 등)
- φ는 조리법: 재료(x)를 어떤 방식으로 요리할지 정하는 방법. 이건 보통 정해져 있거나 바꾸기 어렵다.
    - φ는 입력을 바꿔주는 변환 함수 (특성 변환, 임베딩 등)
